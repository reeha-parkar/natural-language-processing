{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical 6\n",
    "\n",
    "### Read two set of documents, perform tokeization, map dictionaries, create corpus and calculate similarity between the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87648740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd0279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The tiger is the largest living cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ffb5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The tiger is the largest living cat species and a member of the genus Panthera.', 'It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside.', 'It is an apex predator, primarily preying on ungulates such as deer and wild boar.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = sent_tokenize(text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f182648e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'tiger', 'is', 'the', 'largest', 'living', 'cat', 'species', 'and', 'a', 'member', 'of', 'the', 'genus', 'Panthera', '.', 'It', 'is', 'most', 'recognisable', 'for', 'its', 'dark', 'vertical', 'stripes', 'on', 'orange-brown', 'fur', 'with', 'a', 'lighter', 'underside', '.', 'It', 'is', 'an', 'apex', 'predator', ',', 'primarily', 'preying', 'on', 'ungulates', 'such', 'as', 'deer', 'and', 'wild', 'boar', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34006278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars is the fourth planet from the sun. It is the second smallest planet in the solar system after mercury. Saturn is yellow planet.\n",
      "Number of lines =  3\n"
     ]
    }
   ],
   "source": [
    "doc = \"\"\n",
    "length = 0\n",
    "with open('file1.txt', 'r') as f:\n",
    "    doc = f.read()\n",
    "print(doc)\n",
    "sent_tokens = sent_tokenize(doc)\n",
    "print('Number of lines = ', len(sent_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9ec1c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mars', 'is', 'the', 'fourth', 'planet', 'from', 'the', 'sun', '.'], ['it', 'is', 'the', 'second', 'smallest', 'planet', 'in', 'the', 'solar', 'system', 'after', 'mercury', '.'], ['saturn', 'is', 'yellow', 'planet', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words:\n",
    "word_tokens = [[w.lower() for w in word_tokenize(text)] for text in sent_tokens]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "442ad95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'fourth': 1, 'from': 2, 'is': 3, 'mars': 4, 'planet': 5, 'sun': 6, 'the': 7, 'after': 8, 'in': 9, 'it': 10, 'mercury': 11, 'second': 12, 'smallest': 13, 'solar': 14, 'system': 15, 'saturn': 16, 'yellow': 17}\n"
     ]
    }
   ],
   "source": [
    "# Create the dictionary using gensim\n",
    "dictionary = gensim.corpora.Dictionary(word_tokens)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "501f45c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2)], [(0, 1), (3, 1), (5, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(0, 1), (3, 1), (5, 1), (16, 1), (17, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create a corpus/vectorization:\n",
    "corpus = [dictionary.doc2bow(token) for token in word_tokens]\n",
    "print(corpus) # will give (vector_number_given_in_the_step_above, frequency_of_that_word_in_the_sentence) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4c73f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fourth', 0.47], ['from', 0.47], ['mars', 0.47], ['sun', 0.47], ['the', 0.35]] \n",
      "\n",
      "[['the', 0.25], ['after', 0.34], ['in', 0.34], ['it', 0.34], ['mercury', 0.34], ['second', 0.34], ['smallest', 0.34], ['solar', 0.34], ['system', 0.34]] \n",
      "\n",
      "[['saturn', 0.71], ['yellow', 0.71]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the frequency (TFID Term frequency)\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "#print(tf_idf[corpus], '\\n')\n",
    "for doc in tf_idf[corpus]:\n",
    "    #print(doc, '\\n')\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f91e878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity measure\n",
    "sims = gensim.similarities.Similarity('', tf_idf[corpus], num_features=len(dictionary)) # '' will have the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b89a0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saturn is sixth planet from Sun.\n",
      "Number of lines =  1\n"
     ]
    }
   ],
   "source": [
    "query_doc = \"\"\n",
    "with open('file2.txt', 'r') as f1:\n",
    "    query_doc = f1.read()\n",
    "print(query_doc)\n",
    "q_sent_tokens = sent_tokenize(query_doc)\n",
    "print('Number of lines = ', len(q_sent_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f39d4b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saturn', 'is', 'sixth', 'planet', 'from', 'sun', '.']\n",
      "[(0, 1), (2, 1), (3, 1), (5, 1), (6, 1), (16, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Word tokenize it and convert into bag of words\n",
    "for line in q_sent_tokens:\n",
    "    q_word_tokens = [w.lower() for w in word_tokenize(line)]\n",
    "    q_bow = dictionary.doc2bow(q_word_tokens)\n",
    "print(q_word_tokens)\n",
    "print(q_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c633c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.5773502691896258), (6, 0.5773502691896258), (16, 0.5773502691896258)]\n",
      "Comparing results: [0.5416385  0.         0.40824828]\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity query against the original corpus:\n",
    "query_tf_idf = tf_idf[q_bow]\n",
    "print(query_tf_idf)\n",
    "print('Comparing results:', sims[query_tf_idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea4f64f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9498868\n"
     ]
    }
   ],
   "source": [
    "# Adding the similarities\n",
    "import numpy as np\n",
    "sum_of_sims =(np.sum(sims[query_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63f8fb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity float: 0.31662893295288086\n",
      "Average similarity percentage: 31.662893295288086\n",
      "Average similarity rounded percentage: 32\n"
     ]
    }
   ],
   "source": [
    "# Getting the similarity percentage:\n",
    "percentage_of_similarity = round(float((sum_of_sims / len(word_tokens)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(word_tokens))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(word_tokens)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
