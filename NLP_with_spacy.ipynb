{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical 3\n",
    "\n",
    "### Read text and perform basic preprocessing techniques on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fb007a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "748bb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac4f40fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When learning data science, you shouldn't get discouraged.\n",
      "Challenges and setbacks aren't failures, they're just a part of the journey. You've got this!\n"
     ]
    }
   ],
   "source": [
    "text = '''When learning data science, you shouldn't get discouraged.\n",
    "Challenges and setbacks aren't failures, they're just a part of the journey. You've got this!'''\n",
    "\n",
    "# nlp object is used to create documents with linguistic annotations\n",
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "001d7618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '.', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'a', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of word tokens\n",
    "import nltk\n",
    "word_tokens = nltk.word_tokenize(doc.text) # does not give '\\n' like the other two methods\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06ae363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '.', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'a', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "# or:\n",
    "token_list = []\n",
    "for token in doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58c5e364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '.', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'a', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "# or:\n",
    "word_list = [word.text for word in doc]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8c51c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When learning data science, you shouldn't get discouraged.\", \"\\nChallenges and setbacks aren't failures, they're just a part of the journey.\", \"You've got this!\"]\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenization:\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "# Create the pipeline sentencizer compeonent\n",
    "#sbd = nlp.create_pipe('sentencizer')\n",
    "#print(sbd)\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "text = '''When learning data science, you shouldn't get discouraged.\n",
    "Challenges and setbacks aren't failures, they're just a part of the journey. You've got this!'''\n",
    "\n",
    "# nlp object is used to create documents with linguistic annotations\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create a list of sentence tokens\n",
    "sentence_list = []\n",
    "for sentence in doc.sents:\n",
    "    sentence_list.append(sentence.text)\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5bfb777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'forty', 'own', 'they', 'fifty', 'side', 'hers', 'it', 'beyond', 'either', 'me', 'become', 'these', 'when', '’d', 'will', 'anyway', 'as', 'perhaps', 'just', 'well', 'though', 'with', 'myself', 'but', 'move', '’ll', 'can', 'hereby', \"'ve\", 'n‘t', 'cannot', 'between', \"'ll\", '’s', 'everywhere', 'during', 'whereafter', 'its', 'i', 'less', 'behind', 'around', 'see', 'throughout', 'made', 'does', 'hence', 'twenty', 'former', 'back', 'noone', 'sometimes', 'am', 'namely', 'sometime', 'be', 'whom', 'ever', 'this', 'via', 'such', 'whereas', 'some', 'ten', 'than', 'into', 'same', 'already', 'down', 'he', 'hereafter', 'against', 'might', 'upon', 'therefore', 'onto', 'being', 'him', 'to', 'front', 'until', 'became', 'amongst', 'fifteen', 'towards', 'alone', 'eleven', 'without', 'yet', 'not', 'n’t', 'everything', 'somehow', 'along', 'someone', 'sixty', 'others', 'one', 'amount', 'enough', 'no', 'then', 'on', 'except', 'indeed', 'ca', 'seem', '’re', 'very', 'mine', 'meanwhile', 'which', '‘s', 'now', 'up', 'call', 'nine', 'six', '‘m', 'nor', 'nevertheless', 'about', 'among', 'seeming', 'herein', 'go', 'unless', 'therein', 'whose', 'five', 'often', 'always', 'whereby', 'becomes', 'part', 'somewhere', 'his', 'why', 'together', 'their', 'few', 'first', 'the', 'wherever', 'every', 'whoever', 'for', 'most', 'whence', 'while', 'formerly', 'many', 'whether', 'through', 'thus', 'ours', 'us', 'had', 'do', 'yours', '‘d', 're', 'beforehand', 'various', 'another', 'rather', 'yourselves', 'much', \"'d\", 'thereafter', 'out', 'after', 'have', 'thereupon', 'wherein', 'serious', 'bottom', 'put', 'still', 'due', 'seems', 'two', 'although', 'could', 'further', 'if', 'whither', 'doing', 'however', 'would', 'elsewhere', 'within', 'toward', 'anywhere', 'keep', 'them', 'over', 'four', 'mostly', 'may', 'show', 'last', 'our', \"'re\", 'latterly', 'using', 'all', 'her', 'eight', 'otherwise', 'was', 'whatever', 'also', 'give', 'did', '‘ve', 'besides', 'done', 'nobody', 'ourselves', 'seemed', 'by', 'how', 'himself', 'were', 'you', 'or', 'an', 'where', 'herself', 'any', 'beside', 'whereupon', 'again', 'least', 'get', 'next', 'of', 'moreover', 'and', 'everyone', 'take', 'once', 'quite', 'she', 'has', 'across', 'more', 'latter', 'yourself', 'been', '‘ll', 'really', 'is', 'those', 'in', \"n't\", 'who', 'thru', 'under', 'third', 'each', 'there', 'should', 'becoming', 'so', 'nothing', '’m', 'several', 'used', 'anyhow', 'thence', 'both', 'else', 'that', 'make', 'none', 'are', 'from', 'too', 'itself', 'your', 'full', 'never', 'hundred', 'per', 'please', 'afterwards', 'since', 'almost', 'top', \"'m\", '’ve', 'name', 'only', 'here', 'neither', 'at', 'because', 'hereupon', '‘re', 'above', 'my', 'twelve', 'a', 'regarding', 'whenever', \"'s\", 'off', 'empty', 'something', 'other', 'what', 'whole', 'must', 'we', 'anyone', 'before', 'even', 'three', 'say', 'below', 'nowhere', 'anything', 'thereby', 'themselves'}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stopwords are those words that do not contribute to the processing of the language\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(spacy_stopwords)\n",
    "print('when' in spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a24c9629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learning', 'data', 'science', ',', 'discouraged', '.', '\\n', 'Challenges', 'setbacks', 'failures', ',', 'journey', '.', 'got', '!']\n"
     ]
    }
   ],
   "source": [
    "# Filter the word tokens such that there are no stop words\n",
    "list_without_stopwords = []\n",
    "for word in token_list:\n",
    "    wordx = word.lower() # if we don't do this, When will be there, but it is a stopword, so turn into lower case\n",
    "    if wordx not in spacy_stopwords:\n",
    "        list_without_stopwords.append(word)\n",
    "print(list_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fee1aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[learning, data, science, ,, discouraged, ., \n",
      ", Challenges, setbacks, failures, ,, journey, ., got, !]\n"
     ]
    }
   ],
   "source": [
    "# or:\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "filtered_sent = []\n",
    "doc = nlp(text)\n",
    "for word in doc:\n",
    "    if word.is_stop == False:\n",
    "        filtered_sent.append(word)\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2304dcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run \n",
      "runner \n",
      "running \n",
      "runs \n"
     ]
    }
   ],
   "source": [
    "# Implementing Lemmatization\n",
    "lem = nlp('run runner running runs')\n",
    "\n",
    "# Finding lemma for each word:\n",
    "for word in lem:\n",
    "    print(word.text, word.lemma_) # word.lemma_ not printing ?!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
