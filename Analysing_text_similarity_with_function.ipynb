{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f33fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adb95931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(filename):\n",
    "    doc = \"\"\n",
    "    length = 0\n",
    "    with open(filename) as f:\n",
    "        doc = f.read()\n",
    "    #print(doc)\n",
    "    sent_tokens = sent_tokenize(doc)\n",
    "    #print('Number of lines = ', len(sent_tokens))\n",
    "\n",
    "    # Tokenize words:\n",
    "    word_tokens = [[w.lower() for w in word_tokenize(text)] for text in sent_tokens]\n",
    "    #print(word_tokens)\n",
    "\n",
    "    # Create the dictionary using gensim\n",
    "    dictionary = gensim.corpora.Dictionary(word_tokens)\n",
    "    #print(dictionary.token2id)\n",
    "\n",
    "    # Create a corpus/vectorization:\n",
    "    corpus = [dictionary.doc2bow(token) for token in word_tokens]\n",
    "    #print(corpus) # will give (vector_number_given_in_the_step_above, frequency_of_that_word_in_the_sentence) format\n",
    "    \n",
    "    return (word_tokens, dictionary, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d749b88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2)], [(0, 1), (3, 1), (5, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(0, 1), (3, 1), (5, 1), (16, 1), (17, 1)]]\n"
     ]
    }
   ],
   "source": [
    "(word_tokens, dictionary, corpus_file1) = processing('file1.txt')\n",
    "print(corpus_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "695a12d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fourth', 0.47], ['from', 0.47], ['mars', 0.47], ['sun', 0.47], ['the', 0.35]] \n",
      "\n",
      "[['the', 0.25], ['after', 0.34], ['in', 0.34], ['it', 0.34], ['mercury', 0.34], ['second', 0.34], ['smallest', 0.34], ['solar', 0.34], ['system', 0.34]] \n",
      "\n",
      "[['saturn', 0.71], ['yellow', 0.71]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the frequency (TFID Term frequency)\n",
    "tf_idf = gensim.models.TfidfModel(corpus_file1)\n",
    "#print(tf_idf[corpus_file1], '\\n')\n",
    "for doc in tf_idf[corpus_file1]:\n",
    "    #print(doc, '\\n')\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6d8dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity measure\n",
    "sims = gensim.similarities.Similarity('', tf_idf[corpus_file1], num_features=len(dictionary)) \n",
    "# '' will have the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3048511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['saturn', 'is', 'sixth', 'planet', 'from', 'sun', '.']]\n",
      "[(0, 1), (2, 1), (3, 1), (5, 1), (6, 1), (16, 1)]\n"
     ]
    }
   ],
   "source": [
    "(q_word_tokens, dictionary2, corpus_query_doc) = processing('file2.txt')\n",
    "print(q_word_tokens)\n",
    "for line in q_word_tokens:\n",
    "    q_bow = dictionary.doc2bow(line)\n",
    "print(q_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6da7b761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.5773502691896258), (6, 0.5773502691896258), (16, 0.5773502691896258)]\n",
      "Comparing results: [0.5416385  0.         0.40824828]\n",
      "0.9498868\n",
      "Average similarity float: 0.31662893295288086\n",
      "Average similarity percentage: 31.662893295288086\n",
      "Average similarity rounded percentage: 32\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity query against the original corpus:\n",
    "query_tf_idf = tf_idf[q_bow]\n",
    "print(query_tf_idf)\n",
    "print('Comparing results:', sims[query_tf_idf])\n",
    "\n",
    "# Adding the similarities\n",
    "import numpy as np\n",
    "sum_of_sims =(np.sum(sims[query_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)\n",
    "\n",
    "# Getting the similarity percentage:\n",
    "percentage_of_similarity = round(float((sum_of_sims / len(word_tokens)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(word_tokens))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(word_tokens)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967866c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
